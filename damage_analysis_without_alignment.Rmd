---
title: "de_novo2"
author: "Jacqueline Rehn"
date: "8/11/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#load packages
library(dplyr)
library(tibble)
library(readr)
library(reshape2)
library(stringr)
library(ggplot2)
library(gridExtra)
library(pander)

#set aesthetics for plots
theme_set(theme_bw())
palette <- c("#FF3333", "#3333FF", "#009900", "#FF9900", "#990099", 
             "#33CCCC", "#66CC66", "#FFCC66", "#FF99CC", "#3399FF", 
             "#FF6666", "#9966FF")
```


## Detecting aDNA damage

DNA is known to degrade over time. This typically results in short fragments of DNA (<100bp) and cytosine deamination to uracil at the ends of the fragments. When sequenced, the presence of uracil results in an increase in a cytosine to thymine substitutions. When these sequenced reads are aligned against a modern reference sequence this damage is observed as an increased frequency of C->T mutations at the 5' end of the read and G->A at the 3' end. These patterns of damage can be quantified using the program mapDamage2.0, however this process is dependent on ability to align aDNA sequences against a reference genome. There are two instances where this is particularly difficult:

1. The sequences have been produced from a complex metagenomic sample such as dental calculus where it is difficult to identify what microbial species. Due to sequence conservation between microbial species, aligment of the sequenced data against modern reference genomes using bwa results will result in spurious alignments, inflating/deflating damage estimates.

2. When aDNA sequences have been produced from biological material for which no modern reference currently exists and thus it is not possible to align the sequences and detect frequency of substitutions.

In these cases it would be useful to estimate the amount of damage present in the aDNA reads using a method that does not rely on alignment to a reference genome. One way this may be accomplished is by plotting the length distribution of reads directly from the fastq files and also calculating the proportions of A,T,C and G at each position along the read. If uracil was present at the ends of fragments and resulted in miscoding lesions it is expected that we would observe an increase in the proportion of T at the 5' ends of reads and an increase in the proportion of A at the 3' ends.

## Simulated Data

30 Simulated datasets were provided by Raphael Eisenhofer from ACAD which had been previously used to benchmark MALT and MALTx. Each dataset contained 1.5 million reads generated by fragmenting 29 bacterial genomes (25 common plaque bacteria and 4 common environmental/laboratory contaminant bacteria) in gargamel. The table and plot below describe the Taxons included in all datasets and their abundance.

```{r simulatedMetagenomes, message=FALSE}

#import data regarding summarising genomes used to simulate datasets
simData <- read_csv("SimulatedMetagenome.csv", col_names = FALSE, col_types = "c-n-c-cn--", skip = 1)
colnames(simData) <- c("taxon", "abundance", "genus", "contaminant", "GC")
simData <- simData %>% filter(abundance < 1)

#Present data in table
simData %>% pander(caption = "Summary of genome abundances used in all simulated datasets")

#Calculate abundance for each genome and plot as a barchart, indicating if genus origin is oral or contaminant
genusAbundance <- simData %>% select(-taxon, -contaminant, -GC) %>% group_by(genus) %>% summarise_each(funs(sum))
genusAbundance <- simData %>% select(genus, contaminant) %>% unique() %>% left_join(genusAbundance)
genusAbundance %>% 
  ggplot(aes(x=genus, y=abundance, fill=contaminant)) + 
  geom_bar(stat = "identity", colour = "white", position = position_dodge()) + 
  labs(x="", y="Abundance", title="Simulated abundance by genus") + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.2)) + 
  scale_fill_manual(values = c("#FF6666", "#3399FF"), 
                      name = "", 
                      breaks=c("FALSE", "TRUE"), 
                      labels=c("Oral bacteria", "Common contaminant"))

```

Datasets of various lengths and damage profiles were simulated. Fixed read lengths of 30, 50, 70 and 90bp were simulated as well as an empirical ancient DNA fragment length distribution (Empirical). For each read length profile a damaged (suffix _d.fa.gz) and undamaged (suffix .b.fa.gz) data set was simulated. For damaged sets deamination rates for single-stranded overhangs were simulated at either 10%, 50% or ~20% (Real-damage-profile). 

## Fastq counts

The following bash script was used to collate fragment lengths as well as the proportion of each base at the first 10 and last 10 positions in the sequences.

```{bash eval=FALSE}
#!/bin/bash

#Assess fragment length of reads and calculate proportion of each base at both ends of fastq reads

#Specify variables
ROOTDIR=/home/a1698312
TRIMDIR=$ROOTDIR/simData/data
LOGFILE=$ROOTDIR/simData/data/damageAnalysisLog.txt

################ Count read lengths ######################

#Change into directory where trimmed_fastq files located
if [ -d ${TRIMDIR} ]
then
  echo "Changing to trimData directory"
  cd ${TRIMDIR}
else
  echo "Cannot find ${TRIMDIR}"
exit1
fi

##### create log file for script ####

if [ ! -f damageAnalysisLog.txt ]
then
  echo -e 'Creating file damageAnalysisLog.txt'
  echo -e "file\ttime" > ${LOGFILE}
else
  echo -e 'damageAnalysisLog.txt already exists'
fi

#Generate text file for storing length count data
if [ ! -f fastq_length.txt ]
then
  echo -e 'Creating file fastq_length.txt'
  echo -e 'occ\tlength' > fastq_length.txt
else
  echo  'fastq_length.txt already exists'
fi

#Generate text file for storing base proportions
if [ ! -f new_base_proportions.txt ]
then
  echo -e 'Creating file new_base_proportions.txt'
  echo -e "fileName\tend\tpos\tA\tC\tG\tT" > new_base_proportions.txt
else
  echo  'new_base_proportions.txt already exists'
fi

#Count fastq lengths and proportion of each base for each file

for fastq_file in *fa.gz
  do
    #variable to start timing loop
    STARTTIME=$(date +%s)
    echo -e "Collating read lengths in ${fastq_file}"
    
    #extract DNA sequence from file, count length of each sequence, sort lengths and count number of unique
    LENGTH=$(zcat ${fastq_file} | sed -n '2~2p' | awk '{print length($1)}' | sort -n | uniq -c)
    
    #print fileName and collated lengths to bottom of fastq_length.txt
    echo -e "${fastq_file}"  >> fastq_length.txt
    echo -e "${LENGTH}"  >> fastq_length.txt
    
    #set counter to 1
    n=1
    
    echo -e "Collating base proportions in ${fastq_file}"    
    
    #loop which increases value of n(counter) each time up to 25
    while [ $n -le 25 ]
    do

      #extract DNA sequence from file, print base at position n in sequence, count number of each base
      COUNT5p=$(zcat ${fastq_file} | sed -n '2~2p' | awk 'length($0) > '"$(($n*2))"'' | gawk -F '' '{print $'"$n"'}' | \
      sort | uniq -c | awk '{print}' ORS='\t')
      
      echo -e "${fastq_file}\t5p\t${n}\t${COUNT5p}" >> new_base_proportions.txt

      #extract DNA sequences, reverse them (so now reading from 3' end of read), print base at position n, count number of each base
      COUNT3p=$(zcat ${fastq_file} | sed -n '2~2p' | awk 'length($0) > '"$(($n*2))"'' | rev | gawk -F '' '{print $'"$n"'}' | \
      sort | uniq -c | awk '{print}' ORS='\t')
      
      echo -e "${fastq_file}\t3p\t${n}\t${COUNT3p}" >> new_base_proportions.txt
      
      #add 1 to the counter
      let n=n+1
      
      #end inner/while loop
    done
    
    #calculate time taken to complete analysis on this file
    ENDTIME=$(date +%s)
    echo -e "${fastq_file}\t$(($ENDTIME - $STARTTIME))" >> ${LOGFILE}
    
  #end outer/for loop
  done

```

## Length Distributions

The length distribution for all simulated fastq files was initially visualised as a line graph. As most of the simulated data files contained fragments with a specified length, only the Empirical damage profiles are included below:

```{r length distributions, echo=FALSE}

############## Read length distributions ################

#make a list of fastq files
fastqFiles <- list.files("data/", pattern = "fa.gz$", full.names = FALSE)

#Read-in text file fastq_length.txt and assign to object
fastqLength <- read.csv(file="data/fastq_length.txt", 
                        sep="", skip = 1, header = FALSE, 
                        col.names = c("occ", "length"))

#Split at .fa.gz to generate a list
fastqLength <- fastqLength %>% 
  mutate(fa = grepl("fa", occ), fileNo = cumsum(fa)) %>% 
  split(f = .$fileNo)

#assign this list of fastq files as names of files in fastqLength
names(fastqLength) <- fastqFiles

#Bind_rows of list, taking list names and re-inserting as fileName, then remove unnecessary information
fastqLength <- fastqLength %>% 
  bind_rows(.id = "fileName") %>% 
  select(-fa, -fileNo) %>% filter(length != "NA")

#edit fileName to remove unnecessary information
fastqLength$fileName <- gsub('-no-adapters', '', fastqLength$fileName)
fastqLength$fileName <- gsub('fa.gz', '', fastqLength$fileName)
fastqLength$fileName <- gsub('-ACAD-adapters', '', fastqLength$fileName)

#Convert fastqLength variable from factor to numeric
fastqLength <- fastqLength %>% mutate_if(is.factor, as.character)
fastqLength$occ <- as.numeric(fastqLength$occ)

#Plot the length distributions taken from the fastq files
fastqLength %>% subset(grepl("Empirical", fileName)) %>%
  ggplot(aes(x=length, y=occ, colour=fileName)) + 
  geom_line() + 
  labs(x="Read length", y="Number of reads", 
       colour="Sample", title="Length distribution of fastq reads") + 
  scale_colour_manual(values = palette)
```

We observe only a single line, indicating that the length distribution of all 6 simulated files is identical. This can be confirmed by re-plotting the same data using facet_wrap.

```{r echo=FALSE}

fastqLength %>% subset(grepl("Empirical", fileName)) %>%
  ggplot(aes(x=length, y=occ, colour=fileName)) + 
  geom_line() + 
  labs(x="Read length", y="Number of reads", title="Length distribution of fastq reads") + 
  scale_colour_manual(values = palette) + 
  facet_wrap(~fileName) +
  guides(colour=FALSE)

```

## Box plot comparing length distributions for each fastq file

ggplot2 requires data for boxplots to be in the form of a vector of individual lengths. This can be completed using ```r, rep(x$length,x$occ)```. However, for large fastq files this requires too much memory (if 50 million reads this will produce a vector of length 50 million). Instead a function was written to scale the frequency data prior to converting to a vector. This conserves memory and still enables a box-plot to be draw which is representative of the length distributions in the file.

```{r echo=FALSE}

#Write a function to scale the occ by factor of 100 and use scaled value to rep the lengths
scale_lengths <- function(x){
  x <- x %>% split(f = .$fileName) %>% 
    lapply(function(x){
      data_frame(length = rep(x$length,round((x$occ)/1000)))
    }) %>% bind_rows(.id = "fileName")
  return(x)
}

#apply scale_lengths() function to each of the samples
expandedLengths <- scale_lengths(fastqLength)

#Bind each of the scaled lengths vectors into one data_frame and generate boxplot
expandedLengths %>%
  ggplot(aes(x=fileName, y=length, fill=fileName)) + 
  geom_boxplot(outlier.color = "dark grey", outlier.size = 0.3) + 
  theme(axis.title.x = element_blank(), axis.text.x = element_text(angle = 90)) + 
  ylab("Fragment Length") + 
  guides(fill=FALSE) +
 # scale_x_discrete(labels = c(sample_names)) + 
  ggtitle("Fastq read lengths by sample")


```

As indicated, data-sets were simulated to contain specified fragment lengths (30,50,70 or 90bp) or fragment lengths representative of aDNA data (Empirical). The Empirical simulations demonstrate identical length distributions. 

## Summary statistics for length distributions

The number of reads, mean read length and standard deviation for lengths is computed from the frequency data for all fastq files and displayed in the following table. Function written for calculating these stats as converting frequency data to a vector for large fastq files requires too much memory.

```{r echo=FALSE}

##### Create a table summarising key statistics (No.Reads, mean length, standard deviation) ####

#Mutate fastqLength to include count, length.occ, mean, (length-mean)^2*occ
fastqLength <- fastqLength %>% group_by(fileName) %>% 
  mutate(count = sum(occ), length.occ = length*occ, mean = sum(length.occ)/count, start.var = ((length-mean)^2)*occ)

#find count, mean and sd for length.df
length.stats <- fastqLength %>% group_by(fileName) %>% 
  summarise(count = sum(occ), mean = sum(length.occ)/count, sd = sqrt(sum(start.var)/count))

length.stats %>% pander(caption = "Summary statistics for fastq read lengths")

```

## Nucleotide substitution rate

Data sets with fixed fragment lengths were simulated to contain single-stranded overhang cytosine deamination rates of 10% or 50%. Files with a .b suffix are undamaged while those with a _d are damaged. The final two files contain Real-damage-profile of ~20% single-stranded overhang deamination.

The number of each base present in each position for the first and last 10bp of each read was counted and stored in the file base_proportions.txt. This data was imported into R and plotted with the following function.

```{r base proportions, message=FALSE, warning=FALSE}

#read-in text file base_proportions.txt and assign to object
newBaseProp <- read_delim(file = "data/new_base_proportions.txt", delim = "\t", skip = 1, 
                          col_names = c("fileName", "end", "pos", "A", "C", "G", "T"), 
                          col_types = "ccncccc-", na = c(" "))
#remove unnecessary bases listed
newBaseProp$A <- gsub(' A', '', newBaseProp$A)
newBaseProp$C <- gsub(' C', '', newBaseProp$C)
newBaseProp$G <- gsub(' G', '', newBaseProp$G)
newBaseProp$T <- gsub(' T', '', newBaseProp$T)
#convert counts for each base from character to numeric
newBaseProp[4:7] <- lapply(newBaseProp[4:7], as.numeric)

#remove rows with NA
newBaseProp <- na.omit(newBaseProp)

########### Calculate base proportions ############

newBaseProp <- newBaseProp %>% 
  mutate(total = (A + C + G + T)) %>% 
  mutate(T = T/total, G = G/total, C = C/total, A = A/total) %>% 
  select(-total) 

######### Visualise base proportions ##############

# create graphing function
baseProp.graph <- function(df, na.rm = TRUE, ...){
  
  # create list of sampleID's in data to loop over
  fileName_list <- unique(df$fileName)
  
  # create for loop to split data based on sampleID 
  for (i in seq_along(fileName_list)) {
    
    # create object to store 5p data
    baseProp_5p <- subset(df, df$fileName==fileName_list[i]) %>% 
      filter(end == "5p") %>% melt(id.vars = c("fileName", "end", "pos"))
    
    # create object to store 3p data
    baseProp_3p <- subset(df, df$fileName==fileName_list[i]) %>% 
      filter(end == "3p") %>% melt(id.vars = c("fileName", "end", "pos"))
    
    # plot to object, 5p data
    plotBaseProp5p <- baseProp_5p %>% 
      ggplot(aes(x=pos, y=value, colour=variable)) + 
      geom_line() +
      theme_bw() +
      ylim(0.1,0.4) +
      scale_colour_manual(values = c("#FF9900","#3333FF","#009900","#FF3333"), name="Base", breaks=c("T","C","A","G")) + 
      theme(legend.position="none") + 
      labs(x="Position from 5' end of read", y="Proportion of bases")
    
    # plot to object, 3p data   
    plotBaseProp3p <- baseProp_3p %>% 
      ggplot(aes(x=pos, y=value, colour=variable)) + 
      geom_line() +
      theme_bw() +
      scale_y_continuous(limits = c(0.1,0.4), position = "right") +
      scale_x_reverse() +
      scale_colour_manual(values = c("#FF9900","#3333FF","#009900","#FF3333"), name="Base", breaks=c("T","C","A","G")) + 
      labs(x="Position from 3' end of read", y="")
    
    #print plots
    grid.arrange(plotBaseProp5p, plotBaseProp3p, 
                 ncol = 2, 
                 widths=c(0.85,1), 
                 top = (paste(fileName_list[i])))
    
    #End loop
  }
}

```

As there are 30 data-sets it is easier to plot these in groups. Initially all undamaged datasets with a fixed length were plotted.

```{r undamaged with fixed length, message=FALSE}

# run graphing function on results from all undamaged datasets of fixed length
filter(newBaseProp, grepl("damage-no-adapters.b.fa.gz", fileName)) %>% baseProp.graph()

# calculate expected GC and AT content for simulated data sets
expectedGC <- simData %>% select(abundance, GC) %>% 
  mutate(contributingGC = abundance*GC) %>% 
  summarise(overallGC = sum(contributingGC))
expectedAT <- 100 - expectedGC

```

We see in all cases that the proportion of A=T and the proportion of G=C. There is a higher proportion of A/T than G/C, indicating that the genomes used in these simulated data sets have a combined GC content of ~44%. This can be confirmed by calculating the expected GC content of the overall data using the GC content of each genome included multiplied by its %Abundance.

It is important to note that these proporitons would vary depending on the overall GC content of the sample. To confirm that these proportions fit with the simulated data the GC content of each taxon was multiplied by its abundance and the results summed together to give an expected GC content of `r expectedGC`%. Thus the AT content is expected to be `r expectedAT`%.

Damaged datasets for each length profile but with 10% simulated damage are shown below:

```{r message=FALSE}

# run graphing function on results from all data sets with a simulated damage level of 10%
filter(newBaseProp, grepl("_0-1.+_d.fa.gz", fileName)) %>% baseProp.graph()

```

In each of these instances the 5' ends of the reads demonstrate a slight increase in proportion of T and a decrease in the proportion of C, as we would expect if miscoding lesions have occured resulting in a C->T substitution. The exact change in proporiton of bases between the first and tenth positions is summarised in the table.

```{r message=FALSE}

#write above into a function that produces a single data frame from which required results can be extracted.

Calculate_BasePropChange <- function(df, ...){
  
  #extract data for final position in each file
  maxPos <- df %>% split(f = .$end) %>% 
    lapply(function(x){x[cumsum(rle(x$fileName)$lengths), ]}) %>% 
    bind_rows() %>% 
    melt(id.vars = c("fileName", "end", "pos"))
  
  #extract information for position 1 and join to above
  minPos <- df %>% split(f = .$end) %>% 
    lapply(function(x){x %>% filter(pos == "1")}) %>% 
    bind_rows() %>% 
    melt(id.vars = c("fileName", "end", "pos"))
  
  #calculate the change in base proportion
  change_in_prop <- left_join(minPos, maxPos, by = c("fileName", "end", "variable")) %>% 
    mutate(change = (value.x - value.y)*100) %>% 
    select(fileName, end, variable, change)
  
  #convert from long to wide format
  change_in_prop <- dcast(change_in_prop, fileName + end ~ variable, value.var = "change")
    
  #round values to 4 decimal places
  change_in_prop <- change_in_prop %>% mutate_each(funs(round(.,2)), T,A,C,G)
  
  #return change_in_prop
  return(change_in_prop)
}

#view table
filter(newBaseProp, grepl("_0-1.+_d.fa.gz", fileName)) %>% 
  Calculate_BasePropChange() %>% 
  pander("Percentage change in base proportions")

```

There has been ~1.55% increase in the proportion of T bases between position 10 and position 1 of the 5p' end of reads and a similar decrease in the proportion of C bases across the same region, indicative of C -> T miscoding lesions. At the 3p' end the same pattern is observed except we see G -> A mutations as a result of the library preparation process. 

Thus a 10% deamination rate appears to result in ~1.55% change in proportion of bases, regardless of strand length. It would therefore be expected that a 50% deamination rate would result in an ~7.75% change in proportion of bases at the ends of reads.

```{r message=FALSE}

# run graphing function on results from all damaged datasets with a deamination rate of 50%
filter(newBaseProp, grepl("_0-5.+_d.fa.gz", fileName)) %>% baseProp.graph()

```

As expected there has been a change in the proportion of bases at the ends of the reads of ~8% with C -> T at 5' end and G -> A at 3'. 

```{r message=FALSE}

#generate table displaying calculated change in base proportions for 50% damage
filter(newBaseProp, grepl("_0-5.+_d.fa.gz", fileName)) %>% 
  Calculate_BasePropChange() %>% 
  pander("Percentage change in base proportions")

```

The final data sets were simulated using an empirically determine deamination rate of ~20% based on samples held at ACAD. This is predicted to result in an ~3% change in the proportion of C/T at 5' end and G/A at 3' end.

```{r message=FALSE}
#plot graphs on real-damage-profile data sets
filter(newBaseProp, grepl("_Real-damage-profile.+_d.fa.gz", fileName)) %>% baseProp.graph()

```


In these profiles the damage is restricted to the ends (last 5-6 bp), rather than expanding further along the read, even for data set with longer simulated lengths. There has been a ~3% change in the proportion of bases at the ends of the reads as predicted, indicative of ~20% deamination rate. 

```{r message=FALSE}

#generate table displaying calculated change in base proportions for 50% damage
filter(newBaseProp, grepl("_Real-damage-profile.+_d.fa.gz", fileName)) %>% 
  Calculate_BasePropChange() %>% 
  pander("Percentage change in base proportions")

```

Calculate mean change for each base at each end of the read and plot.

```{r message = FALSE}

#calculate mean change in proportion of each base for each end and each simulated deamination level
mean_change <- filter(newBaseProp, grepl("_d.fa.gz", fileName)) %>% 
  Calculate_BasePropChange() %>% 
  mutate(damageRate = str_extract(fileName, "(0-1|0-5|Real)")) %>% 
  split(f = .$end) %>% lapply(function(x){x %>% select(-fileName, -end) %>% group_by(damageRate) %>% 
      summarise_each(funs(mean))}) %>% bind_rows(.id = "end")

#replace factor variable (damageRate) with numeric estimates
mean_change$damageRate <- gsub('0-1', '10', mean_change$damageRate)
mean_change$damageRate <- gsub('0-5', '50', mean_change$damageRate)
mean_change$damageRate <- gsub('Real', '20', mean_change$damageRate)
mean_change$damageRate <- as.numeric(as.character(mean_change$damageRate))

#plot mean change by deamination rate using facet_wrap

#create labels for facets
facet_labels <- c(`3p` = "3' end", `5p` = "5' end")
#change order of facets by converting end to factor
mean_change$end_f = factor(mean_change$end, levels=c('5p','3p'))
#generate plot
mean_change %>% melt(id.vars = c("end", "damageRate", "end_f")) %>% 
  ggplot(aes(x=damageRate, y=value, colour=variable)) + 
  geom_line() + 
  facet_wrap(~end_f, labeller = as_labeller(facet_labels)) + 
  scale_colour_manual(values = c("#FF9900","#3333FF","#009900","#FF3333"), name="Base", breaks=c("T","C","A","G")) + 
  labs(x="Simulated % deamination", y="Mean % change in base proportion", colour="Base")

```

From the above plot it appears that there is a linear relationship between the rate of simulated deamination and the mean % change in base proportions. This however has been generated using only a very small testing range(deamination rate of 10%, 50% and Real or ~20%). To confirm this trend additional simulated data sets with alternate deamination rates need to be generated and plotted. 

Other issues not considered in this analysis:

- Effect of alternative GC content for simulated data sets on the % change in base proportion
- Effect of including modern contaminant sequences that are not deaminated on the overall detected damage rate

## Session Info

```{r}
sessionInfo()
```


